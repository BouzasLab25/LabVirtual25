library("difR", lib.loc="~/R/R-3.5.0/library")
data("verbal")
Mantel_Haeszen <- difMH(Data=verbal, group="Gender",
focal.name=1, purify=TRUE, nrIter=20)
print(Mantel_Haeszen)
plot(Mantel_Haeszen, ylim=c(0,12))
#########################################################
#####  DIF analysis an the difR package
#####  Reference:
#####             Title: A general framework and an R package for the detection of dichotomous differential item functioning
#####             Authors: David Magis, Sébastien Meland, Francis Tuerlinckx & Paul De Boeck
rm(list=ls())
library("difR", lib.loc="~/R/R-3.5.0/library")
data("verbal")
Mantel_Haeszen <- difMH(Data=verbal, group="Gender",
focal.name=1, purify=TRUE, nrIter=20)
print(Mantel_Haeszen)
plot(Mantel_Haeszen, ylim=c(0,12))
#########################################################
#####  DIF analysis an the difR package
#####  Reference:
#####             Title: A general framework and an R package for the detection of dichotomous differential item functioning
#####             Authors: David Magis, Sébastien Meland, Francis Tuerlinckx & Paul De Boeck
rm(list=ls())
library("difR", lib.loc="~/R/R-3.5.0/library")
data("verbal")
Mantel_Haeszen <- difMH(Data=verbal, group="Gender",
focal.name=1, purify=TRUE, nrIter=20)
print(Mantel_Haeszen)
plot(Mantel_Haeszen, ylim=c(0,12))
res.MH<- difMH(Data=verbal,
group="Gender", focal.name=1,
purify=TRUE, nrIter=20) plot(res.MH)
res.MH<- difMH(Data=verbal,
group="Gender", focal.name=1,
purify=TRUE, nrIter=20) plot(res.MH)
#########################################################
#####  DIF analysis an the difR package
#####  Reference:
#####             Title: A general framework and an R package for the detection of dichotomous differential item functioning
#####             Authors: David Magis, Sébastien Meland, Francis Tuerlinckx & Paul De Boeck
rm(list=ls())
library("difR", lib.loc="~/R/R-3.5.0/library")
data("verbal")
Mantel_Haeszen <- difMH(Data=verbal, group="Gender",
focal.name=1, purify=TRUE, nrIter=20)
print(Mantel_Haeszen)
res.MH<- difMH(Data=verbal,
group="Gender", focal.name=1,
purify=TRUE, nrIter=20)
plot(res.MH)
#########################################################
#####  DIF analysis an the difR package
#####  Reference:
#####             Title: A general framework and an R package for the detection of dichotomous differential item functioning
#####             Authors: David Magis, Sébastien Meland, Francis Tuerlinckx & Paul De Boeck
rm(list=ls())
library("difR", lib.loc="~/R/R-3.5.0/library")
data("verbal")
Mantel_Haeszen <- difMH(Data=verbal, group="Gender",
focal.name=1, purify=TRUE, nrIter=20)
print(Mantel_Haeszen)
plot(Mantel_Haeszen)
View(data)
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.01,.29,.25,.1)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.25,.2,.15) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
sqrt(9)
sqrt(.35)
.35*.35
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.01,.29,.25,.1)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.25,.2,.15) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.15,.2,.25,.4)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.25,.2,.15) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.1,.1,.4,.4)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.4,.1,.1) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
scores
max(scores)
min(scores)
4*20
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.1,.1,.8,.6)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.4,.1,.1) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
scores
length(scores)
# clears workspace:
rm(list=ls())
# sets working directories:
setwd("D:/afchavez/Desktop/Adrifelcha_Lab25/Proyectos/Uri Tesis/CaseStudies/PsychophysicalFunctions")
library(R2jags)
install.packages("R2jags")
# clears workspace:
rm(list=ls())
# sets working directories:
setwd("D:/afchavez/Desktop/Adrifelcha_Lab25/Proyectos/Uri Tesis/CaseStudies/PsychophysicalFunctions")
library(R2jags)
install.packages("rjags")
# clears workspace:
rm(list=ls())
# sets working directories:
setwd("D:/afchavez/Desktop/Adrifelcha_Lab25/Proyectos/Uri Tesis/CaseStudies/PsychophysicalFunctions")
library(R2jags)
install.packages("shiny")
library(shiny); runApp('D:/afchavez/Desktop/LabVirtual25/AppsFelisa/SDT.R')
install.packages("shinydashboard")
runApp('D:/afchavez/Desktop/LabVirtual25/AppsFelisa/SDT.R')
runApp('D:/afchavez/Desktop/LabVirtual25/AppsFelisa/SDT.R')
